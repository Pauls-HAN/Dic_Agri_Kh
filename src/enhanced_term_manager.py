#!/usr/bin/env python3
"""
ìº„ë³´ë””ì•„ ë†ì—…ìš©ì–´ 8000ë‹¨ì–´ í™•ì¥ ê´€ë¦¬ ì‹œìŠ¤í…œ
Enhanced Agricultural Terms Manager for Mobile Learning App
"""

import json
import os
from datetime import datetime
from typing import List, Dict, Optional, Any
import random
import urllib.parse

class EnhancedAgriculturalTermManager:
    def __init__(self, data_file_path: str = None):
        """í™•ì¥ëœ ë†ì—…ìš©ì–´ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        # í™•ì¥ëœ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë¨¼ì € ì •ì˜
        self.categories = [
            "ì‘ë¬¼ì¬ë°°", "ì¶•ì‚°ì—…", "ë†ê¸°ê³„", "í† ì–‘ê´€ë¦¬", "ë¹„ë£Œ", "ë³‘í•´ì¶©ë°©ì œ",
            "ìˆ˜í™•í›„ì²˜ë¦¬", "ì €ì¥ê¸°ìˆ ", "ê°€ê³µê¸°ìˆ ", "ìœ í†µ", "ë†ì—…ì •ì±…", "ë†ì—…ê²½ì˜",
            "ì›ì˜ˆ", "ì„ì—…", "ìˆ˜ì‚°ì—…", "ë†ì—…ê¸°ìˆ ", "ìˆ˜ìì›ê´€ë¦¬", "ë†ì—…ì‹œì„¤",
            "ì¢…ìê¸°ìˆ ", "ë†ì•½", "ìœ ê¸°ë†ì—…", "ìŠ¤ë§ˆíŠ¸ë†ì—…", "ë†ì—…í™˜ê²½", "ê¸°í›„ë³€í™”ëŒ€ì‘",
            "ë†ì´Œê°œë°œ", "ë†ì—…êµìœ¡", "ë†ì—…ê¸ˆìœµ", "ë†ì—…ë³´í—˜", "ë†ì‚°ë¬¼í’ˆì§ˆ", "ë†ì—…ì•ˆì „"
        ]
        
        if data_file_path is None:
            self.data_file_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'enhanced_agricultural_terms.json')
        else:
            self.data_file_path = data_file_path
        
        self.data = self._load_data()
    
    def _load_data(self) -> Dict[str, Any]:
        """ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.data_file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            # ì´ˆê¸° í™•ì¥ëœ ë°ì´í„° êµ¬ì¡° ìƒì„±
            initial_data = {
                "metadata": {
                    "version": "2.0",
                    "total_terms": 0,
                    "last_updated": datetime.now().isoformat(),
                    "target_count": 8000,
                    "daily_learning_size": 10,
                    "categories": self.categories
                },
                "terms": []
            }
            self._save_data(initial_data)
            return initial_data
        except json.JSONDecodeError as e:
            raise Exception(f"ë°ì´í„° íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: {e}")
    
    def _save_data(self, data: Dict[str, Any] = None) -> None:
        """ë°ì´í„° íŒŒì¼ ì €ì¥"""
        if data is None:
            data = self.data
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        data["metadata"]["total_terms"] = len(data["terms"])
        data["metadata"]["last_updated"] = datetime.now().isoformat()
        
        # ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ëŠ” ê²½ìš°)
        os.makedirs(os.path.dirname(self.data_file_path), exist_ok=True)
        
        with open(self.data_file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def add_enhanced_term(self, 
                         korean_term: str,
                         khmer_term: str,
                         khmer_pronunciation: str,
                         category: str,
                         korean_definition: str,
                         khmer_definition: str,
                         korean_example: str,
                         khmer_example: str,
                         khmer_example_pronunciation: str,
                         english_term: str = "",
                         english_example: str = "",
                         image_url: str = "",
                         frequency_level: int = 3,
                         difficulty_level: str = "ì¤‘ê¸‰",
                         tags: List[str] = None,
                         mnemonics: str = "",
                         cultural_notes: str = "") -> int:
        """í™•ì¥ëœ ë†ì—…ìš©ì–´ ì¶”ê°€"""
        
        if tags is None:
            tags = []
        
        # ìƒˆ ID ìƒì„± (ê¸°ì¡´ ìµœëŒ€ ID + 1)
        existing_ids = [term.get("id", 0) for term in self.data["terms"]]
        new_id = max(existing_ids, default=0) + 1
        
        # í•™ìŠµ ìˆœì„œ ê³„ì‚° (frequency_level ê¸°ë°˜)
        learning_order = self._calculate_learning_order(frequency_level, difficulty_level)
        
        new_term = {
            "id": new_id,
            "korean_term": korean_term,
            "khmer_term": khmer_term,
            "khmer_pronunciation": khmer_pronunciation,
            "english_term": english_term,
            "category": category,
            "korean_definition": korean_definition,
            "khmer_definition": khmer_definition,
            "korean_example": korean_example,
            "khmer_example": khmer_example,
            "khmer_example_pronunciation": khmer_example_pronunciation,
            "english_example": english_example,
            "image_url": image_url,
            "audio_url_khmer": "",  # TTSë¡œ ìƒì„± ì˜ˆì •
            "audio_url_korean": "", # TTSë¡œ ìƒì„± ì˜ˆì •
            "frequency_level": frequency_level,
            "learning_order": learning_order,
            "related_terms": [],
            "difficulty_level": difficulty_level,
            "tags": tags,
            "mnemonics": mnemonics,
            "cultural_notes": cultural_notes,
            "created_date": datetime.now().isoformat(),
            "updated_date": datetime.now().isoformat(),
            "verified": False,
            "last_reviewed": None
        }
        
        self.data["terms"].append(new_term)
        self._save_data()
        
        return new_id
    
    def _calculate_learning_order(self, frequency_level: int, difficulty_level: str) -> int:
        """í•™ìŠµ ìˆœì„œ ê³„ì‚° (ë¹ˆë„ì™€ ë‚œì´ë„ ê¸°ë°˜)"""
        # ê¸°ë³¸ ì ìˆ˜
        base_score = 0
        
        # ë¹ˆë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë¨¼ì € í•™ìŠµ)
        frequency_score = (6 - frequency_level) * 1000  # 5=1000, 4=2000, 3=3000, 2=4000, 1=5000
        
        # ë‚œì´ë„ ì ìˆ˜ (ê¸°ì´ˆ ë¨¼ì €, ê³ ê¸‰ ë‚˜ì¤‘ì—)
        difficulty_scores = {"ê¸°ì´ˆ": 0, "ì¤‘ê¸‰": 3000, "ê³ ê¸‰": 6000}
        difficulty_score = difficulty_scores.get(difficulty_level, 3000)
        
        # í˜„ì¬ ìš©ì–´ ìˆ˜ì— ë”°ë¥¸ ìˆœì„œ
        current_count = len(self.data["terms"])
        
        return frequency_score + difficulty_score + current_count
    
    def get_daily_words(self, day: int, limit: int = 10) -> List[Dict[str, Any]]:
        """ì¼ì¼ í•™ìŠµìš© ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°"""
        start_index = (day - 1) * limit
        
        # í•™ìŠµ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        sorted_terms = sorted(self.data["terms"], key=lambda x: x.get("learning_order", 999999))
        
        # í•´ë‹¹ ë‚ ì§œì˜ ë‹¨ì–´ë“¤
        daily_words = sorted_terms[start_index:start_index + limit]
        
        return daily_words
    
    def get_words_by_category(self, category: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°"""
        filtered_words = [term for term in self.data["terms"] if term.get("category") == category]
        
        # í•™ìŠµ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        sorted_words = sorted(filtered_words, key=lambda x: x.get("learning_order", 999999))
        
        if limit:
            return sorted_words[:limit]
        return sorted_words
    
    def search_enhanced_terms(self, 
                            keyword: str = "", 
                            category: str = "", 
                            difficulty_level: str = "",
                            frequency_level: int = 0,
                            verified_only: bool = False,
                            limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """í™•ì¥ëœ ìš©ì–´ ê²€ìƒ‰"""
        results = []
        keyword_lower = keyword.lower()
        
        for term in self.data["terms"]:
            # ê²€ì¦ëœ ìš©ì–´ë§Œ í•„í„°ë§
            if verified_only and not term.get("verified", False):
                continue
                
            # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            if category and term.get("category", "") != category:
                continue
                
            # ë‚œì´ë„ í•„í„°ë§
            if difficulty_level and term.get("difficulty_level", "") != difficulty_level:
                continue
            
            # ë¹ˆë„ í•„í„°ë§
            if frequency_level > 0 and term.get("frequency_level", 0) != frequency_level:
                continue
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ (ë‹¤ì–‘í•œ í•„ë“œì—ì„œ)
            if keyword:
                searchable_text = " ".join([
                    term.get("korean_term", ""),
                    term.get("khmer_term", ""),
                    term.get("khmer_pronunciation", ""),
                    term.get("english_term", ""),
                    term.get("korean_definition", ""),
                    term.get("khmer_definition", ""),
                    term.get("korean_example", ""),
                    term.get("khmer_example", ""),
                    " ".join(term.get("tags", []))
                ]).lower()
                
                if keyword_lower not in searchable_text:
                    continue
            
            results.append(term)
        
        # í•™ìŠµ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        results.sort(key=lambda x: x.get("learning_order", 999999))
        
        if limit:
            return results[:limit]
        return results
    
    def generate_sample_enhanced_data(self, count: int = 100) -> None:
        """í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        
        sample_base_terms = [
            {
                "korean_term": "ë²¼", "khmer_term": "áŸáŸ’ášá¼áœ", "khmer_pronunciation": "ìŠ¤ë¼ìš°",
                "category": "ì‘ë¬¼ì¬ë°°", "frequency_level": 5,
                "korean_definition": "ë…¼ì—ì„œ ê¸°ë¥´ëŠ” í•œí•´ì‚´ì´ ë²¼ê³¼ ì‹ë¬¼ë¡œ, ìŒ€ì˜ ì›ë£Œê°€ ë˜ëŠ” ê³¡ë¬¼",
                "khmer_definition": "ášá»á€áŸ’áá‡á¶áá·á€áŸ’ášá»á˜á”á¶á™áŠáŸ‚á›áŠá¶áŸ†á€áŸ’á“á»á„áŸáŸ’ášáŸ‚ á“á·á„á‡á¶áœááŸ’áá»á’á¶áá»áŠá¾á˜á“áŸƒá¢á„áŸ’á€áš",
                "korean_example": "ë²¼ë†ì‚¬ëŠ” ìº„ë³´ë””ì•„ì˜ ì£¼ìš” ë†ì—… í™œë™ì…ë‹ˆë‹¤.",
                "khmer_example": "á€á¶ášáŠá¶áŸ†áŸáŸ’ášá¼áœá‚áºá‡á¶áŸá€á˜áŸ’á˜á—á¶á–á€áŸá·á€á˜áŸ’á˜áŸáŸ†áá¶á“áŸ‹ášá”áŸáŸ‹á€á˜áŸ’á–á»á‡á¶",
                "khmer_example_pronunciation": "ì¹´ ë‹´ ìŠ¤ë¼ìš° ë„ ì¹˜ì•„ ì‚¬ê¹œë§ˆíŒŒí”„ ì†œì¹¸ ë¡œë³´ìŠ¤ ê¹œí‘¸ì¹˜ì•„",
                "tags": ["ê³¡ë¬¼", "ì£¼ì‹", "ë…¼ë†ì‚¬"]
            },
            {
                "korean_term": "ë†ê¸°ê³„", "khmer_term": "á‚áŸ’ášá¿á„á™á“áŸ’áá€áŸá·á€á˜áŸ’á˜", "khmer_pronunciation": "í¬ë£½ ìš˜ ê¹Œì‹œê¹œ",
                "category": "ë†ê¸°ê³„", "frequency_level": 4,
                "korean_definition": "ë†ì—… ì‘ì—…ì— ì‚¬ìš©ë˜ëŠ” ê¸°ê³„ì™€ ë„êµ¬ì˜ ì´ì¹­",
                "khmer_definition": "á‚áŸ’ášá¿á„á™á“áŸ’á á“á·á„á§á”á€ášááŸáŠáŸ‚á›á”áŸ’ášá¾á”áŸ’ášá¶áŸáŸ‹áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášá„á¶ášá€áŸá·á€á˜áŸ’á˜",
                "korean_example": "í˜„ëŒ€ì ì¸ ë†ê¸°ê³„ ë„ì…ìœ¼ë¡œ ë†ì‘ì—… íš¨ìœ¨ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.",
                "khmer_example": "á€á¶ášááŸ‚á“á¶áŸ†á‚áŸ’ášá¿á„á™á“áŸ’áá€áŸá·á€á˜áŸ’á˜á‘áŸ†á“á¾á”á’áŸ’áœá¾á±áŸ’á™á”áŸ’ášáŸá·á‘áŸ’á’á—á¶á–á€á¶ášá„á¶ášá€áŸá·á€á˜áŸ’á˜á€á¾á“á¡á¾á„á™áŸ‰á¶á„ááŸ’á›á¶áŸ†á„",
                "khmer_example_pronunciation": "ì¹´ ë‚˜ì— ë‚¨ í¬ë£½ìš˜ ê¹Œì‹œê¹œ í†°ë…¸ì—… íŠ¸ëœ¨ì–´ìœ¼ì´ í”„ë¡œì‹¯íƒ€íŒŒí”„ ì¹´ë‚­ê°€ ê¹Œì‹œê¹œ ê¼¬ì–¸ ë¼ì—¥ ì•¼ì‘ í´ë¼ì‘",
                "tags": ["ê¸°ê³„", "ë„êµ¬", "íš¨ìœ¨ì„±"]
            },
            {
                "korean_term": "ë¹„ë£Œ", "khmer_term": "á‡á¸", "khmer_pronunciation": "ì¹˜",
                "category": "ë¹„ë£Œ", "frequency_level": 5,
                "korean_definition": "ì‹ë¬¼ì˜ ìƒì¥ì— í•„ìš”í•œ ì–‘ë¶„ì„ ê³µê¸‰í•˜ëŠ” ë¬¼ì§ˆ",
                "khmer_definition": "áŸá¶ášá’á¶áá»áŠáŸ‚á›á•áŸ’á‚ááŸ‹á•áŸ’á‚á„áŸ‹áŸá¶ášá’á¶áá»á…á·á‰áŸ’á…á¹á˜á…á¶áŸ†á”á¶á…áŸ‹áŸá˜áŸ’ášá¶á”áŸ‹á€á¶ášá›á¼áá›á¶áŸáŸ‹ášá”áŸáŸ‹ášá»á€áŸ’áá‡á¶áá·",
                "korean_example": "ìœ ê¸°ë¹„ë£Œ ì‚¬ìš©ìœ¼ë¡œ í† ì–‘ì˜ ì§ˆì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "khmer_example": "á€á¶ášá”áŸ’ášá¾á”áŸ’ášá¶áŸáŸ‹á‡á¸áŸášá¸ášá¶á„áŸ’á‚á¢á¶á…á’áŸ’áœá¾á±áŸ’á™á‚á»áá—á¶á–áŠá¸á”áŸ’ášáŸá¾ášá¡á¾á„",
                "khmer_example_pronunciation": "ì¹´ í”„ëŸ¬ì´ í”„ë¼ìŠ¤ ì¹˜ ì‚¬ë¦¬ë‘ ì•„ì¹˜ íŠ¸ëœ¨ì–´ìœ¼ì´ ê¾¼ë‚˜íŒŒí”„ ë‹¤ì´ í”„ë¡œì¨ì–´ ë¼ì—¥",
                "tags": ["ì–‘ë¶„", "í† ì–‘", "ìœ ê¸°ë†"]
            },
            {
                "korean_term": "ìˆ˜í™•", "khmer_term": "á€á¶ášá…áŸ’ášá¼áá€á¶ááŸ‹", "khmer_pronunciation": "ì¹´ ì¸„ë£» ê¹Ÿ",
                "category": "ìˆ˜í™•í›„ì²˜ë¦¬", "frequency_level": 5,
                "korean_definition": "ë†ì‘ë¬¼ì´ ìµì—ˆì„ ë•Œ ê±°ë‘ì–´ë“¤ì´ëŠ” ì‘ì—…",
                "khmer_definition": "á€á¶ášá”áŸ’ášá˜á¼á›á™á€áŠáŸ†áá¶áŸ†á–áŸá›áŠáŸ‚á›áœá¶á‘á»áŸ†á á¾á™",
                "korean_example": "ìŒ€ ìˆ˜í™• ì‹œê¸°ëŠ” ë³´í†µ 11ì›”ë¶€í„° 12ì›”ì…ë‹ˆë‹¤.",
                "khmer_example": "á–áŸá›áœáŸá›á¶á…áŸ’ášá¼áá€á¶ááŸ‹á¢á„áŸ’á€ášá’á˜áŸ’á˜áá¶á…á¶á”áŸ‹á–á¸ááŸ‚áœá·á…áŸ’á†á·á€á¶áŠá›áŸ‹ááŸ‚á’áŸ’á“á¼",
                "khmer_example_pronunciation": "í˜ì¼ ë²¨ë¼ ì¸„ë£» ê¹Ÿ ì‘ê¹Œ í†°ë§ˆë”° ì°¹ í”¼ ì¹´ì— ë¹—ì¹˜ê¹Œ ë‹¬ ì¹´ì— íŠ¸ëˆ„",
                "tags": ["ìˆ˜í™•ê¸°", "ë†ì‘ì—…", "ê³„ì ˆ"]
            },
            {
                "korean_term": "ê´€ê°œ", "khmer_term": "á”áŸ’ášá–áŸá“áŸ’á’á’á¶ášá¶áŸá¶áŸáŸ’ááŸ’áš", "khmer_pronunciation": "í”„ë¡œí‘¼ íƒ€ë¼ì‚¬ìŠ¤",
                "category": "ìˆ˜ìì›ê´€ë¦¬", "frequency_level": 4,
                "korean_definition": "ë†ì‘ë¬¼ì— ë¬¼ì„ ê³µê¸‰í•˜ëŠ” ì¸ê³µì ì¸ ë¬¼ ê³µê¸‰ ì‹œìŠ¤í…œ",
                "khmer_definition": "á”áŸ’ášá–áŸá“áŸ’á’á•áŸ’á‚ááŸ‹á•áŸ’á‚á„áŸ‹á‘á¹á€áŸá·á”áŸ’á”á“á·á˜áŸ’á˜á·ááŸá˜áŸ’ášá¶á”áŸ‹áŠáŸ†áá¶áŸ†á€áŸá·á€á˜áŸ’á˜",
                "korean_example": "íš¨ìœ¨ì ì¸ ê´€ê°œ ì‹œìŠ¤í…œìœ¼ë¡œ ê°€ë­„ í”¼í•´ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "khmer_example": "á”áŸ’ášá–áŸá“áŸ’á’á’á¶ášá¶áŸá¶áŸáŸ’ááŸ’ášá”áŸ’ášá€á”áŠáŸ„á™á”áŸ’ášáŸá·á‘áŸ’á’á—á¶á–á¢á¶á…á€á¶ááŸ‹á”á“áŸ’áá™á€á¶ášáá¼á…áá¶ááŠáŸ„á™áŸá¶á™ášá‚á¶áŸ†á„",
                "khmer_example_pronunciation": "í”„ë¡œí‘¼ íƒ€ë¼ì‚¬ìŠ¤ í”„ë¡œê¹ ë‹¤ì˜¤ì´ í”„ë¡œì‹¯íƒ€íŒŒí”„ ì•„ì¹˜ ê¹Ÿ ë°¤íƒ€ì´ ì¹´ ì¿ ì¹˜ ì¹´íŠ¸ ë‹¤ì˜¤ì´ ì‚¬ì´ë¼ê¹œ",
                "tags": ["ë¬¼", "ì‹œì„¤", "ê°€ë­„ëŒ€ì±…"]
            }
        ]
        
        print(f"í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„° {count}ê°œ ìƒì„± ì¤‘...")
        
        for i in range(count):
            base_term = sample_base_terms[i % len(sample_base_terms)]
            
            # ë³€í˜•ëœ ìš©ì–´ ìƒì„±
            variation = i // len(sample_base_terms) + 1
            
            enhanced_term = {
                **base_term,
                "korean_term": f"{base_term['korean_term']} {variation}",
                "english_term": f"Agricultural term {i+1}",
                "english_example": f"This is an example sentence for agricultural term {i+1}.",
                "difficulty_level": random.choice(["ê¸°ì´ˆ", "ì¤‘ê¸‰", "ê³ ê¸‰"]),
                "frequency_level": random.randint(1, 5),
                "mnemonics": f"{base_term['korean_term']} ì•”ê¸°ë²•: ì—°ìƒì„ í†µí•´ ê¸°ì–µí•˜ì„¸ìš”.",
                "cultural_notes": "ìº„ë³´ë””ì•„ ë†ì—… ë¬¸í™”ì—ì„œ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤."
            }
            
            try:
                self.add_enhanced_term(**enhanced_term)
                if (i + 1) % 20 == 0:
                    print(f"  {i + 1}ê°œ ì™„ë£Œ...")
            except Exception as e:
                print(f"  ì˜¤ë¥˜ ({i+1}ë²ˆì§¸): {e}")
        
        print(f"âœ… {count}ê°œ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
    
    def get_learning_statistics(self) -> Dict[str, Any]:
        """í•™ìŠµìš© í†µê³„ ì •ë³´"""
        total_terms = len(self.data["terms"])
        verified_terms = len([t for t in self.data["terms"] if t.get("verified", False)])
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        frequency_stats = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
        difficulty_stats = {}
        
        for term in self.data["terms"]:
            # ì¹´í…Œê³ ë¦¬ë³„
            category = term.get("category", "ë¯¸ë¶„ë¥˜")
            category_stats[category] = category_stats.get(category, 0) + 1
            
            # ë¹ˆë„ë³„
            frequency = term.get("frequency_level", 3)
            frequency_stats[frequency] += 1
            
            # ë‚œì´ë„ë³„
            difficulty = term.get("difficulty_level", "ì¤‘ê¸‰")
            difficulty_stats[difficulty] = difficulty_stats.get(difficulty, 0) + 1
        
        # ì¼ì¼ í•™ìŠµ í†µê³„
        total_days_needed = (total_terms + 9) // 10  # ì˜¬ë¦¼
        
        return {
            "total_terms": total_terms,
            "verified_terms": verified_terms,
            "unverified_terms": total_terms - verified_terms,
            "target_count": 8000,
            "progress_percentage": round((total_terms / 8000) * 100, 2),
            "category_distribution": category_stats,
            "frequency_distribution": frequency_stats,
            "difficulty_distribution": difficulty_stats,
            "remaining": max(0, 8000 - total_terms),
            "total_learning_days": total_days_needed,
            "current_day": min(total_days_needed, (total_terms + 9) // 10)
        }
    
    def export_for_mobile_app(self, output_path: str) -> bool:
        """ëª¨ë°”ì¼ ì•±ìš© ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        try:
            mobile_data = {
                "app_version": "2.0",
                "data_version": datetime.now().strftime("%Y%m%d"),
                "total_terms": len(self.data["terms"]),
                "categories": self.categories,
                "terms": []
            }
            
            # í•™ìŠµ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ë‚´ë³´ë‚´ê¸°
            sorted_terms = sorted(self.data["terms"], key=lambda x: x.get("learning_order", 999999))
            
            for term in sorted_terms:
                mobile_term = {
                    "id": term.get("id"),
                    "korean": term.get("korean_term", ""),
                    "khmer": term.get("khmer_term", ""),
                    "pronunciation": term.get("khmer_pronunciation", ""),
                    "category": term.get("category", ""),
                    "definition_ko": term.get("korean_definition", ""),
                    "definition_km": term.get("khmer_definition", ""),
                    "example_ko": term.get("korean_example", ""),
                    "example_km": term.get("khmer_example", ""),
                    "example_pronunciation": term.get("khmer_example_pronunciation", ""),
                    "frequency": term.get("frequency_level", 3),
                    "difficulty": term.get("difficulty_level", "ì¤‘ê¸‰"),
                    "learning_order": term.get("learning_order", 999999),
                    "image": term.get("image_url", ""),
                    "tags": term.get("tags", [])
                }
                mobile_data["terms"].append(mobile_term)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(mobile_data, f, ensure_ascii=False, indent=2)
            
            return True
            
        except Exception as e:
            print(f"ëª¨ë°”ì¼ ì•± ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}")
            return False

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë° ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    manager = EnhancedAgriculturalTermManager()
    
    # ê¸°ì¡´ ìš©ì–´ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    if len(manager.data["terms"]) < 50:
        print("ğŸ“š í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
        manager.generate_sample_enhanced_data(100)
    
    # í†µê³„ ì¶œë ¥
    stats = manager.get_learning_statistics()
    print("\n=== ìº„ë³´ë””ì•„ ë†ì—…ìš©ì–´ í•™ìŠµ ì•± í˜„í™© ===")
    print(f"ğŸ“Š ì´ ìš©ì–´ ìˆ˜: {stats['total_terms']:,}")
    print(f"ğŸ¯ ëª©í‘œ ëŒ€ë¹„ ì§„í–‰ë¥ : {stats['progress_percentage']:.1f}%")
    print(f"âœ… ê²€ì¦ ì™„ë£Œ: {stats['verified_terms']:,}")
    print(f"â³ ê²€ì¦ ëŒ€ê¸°: {stats['unverified_terms']:,}")
    print(f"ğŸ“… ì´ í•™ìŠµ ì¼ìˆ˜: {stats['total_learning_days']}ì¼")
    print(f"ğŸ“š í˜„ì¬ í•™ìŠµ ì¼ì°¨: {stats['current_day']}ì¼ì°¨")
    
    print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ (ìƒìœ„ 10ê°œ):")
    for category, count in sorted(stats['category_distribution'].items(), 
                                 key=lambda x: x[1], reverse=True)[:10]:
        print(f"  - {category}: {count:,}ê°œ")
    
    print(f"\nâ­ ë¹ˆë„ë³„ ë¶„í¬:")
    for freq, count in stats['frequency_distribution'].items():
        stars = "â˜…" * freq
        print(f"  - {stars} (ë ˆë²¨ {freq}): {count:,}ê°œ")